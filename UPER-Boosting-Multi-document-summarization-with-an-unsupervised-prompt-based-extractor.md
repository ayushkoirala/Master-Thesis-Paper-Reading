#1. UPER: Boosting Multi-Document Summarization with an Unsupervised Prompt-based Extractor

#2. Paper Link : https://aclanthology.org/2022.coling-1.550.pdf

| Topic | UPER: Boosting Multi-Document Summarization with an Unsupervised Prompt-based Extractor |
| ---------------| --------------------------- |
| Problem | The problem addressed in the paper is the limitations and drawbacks of existing methods used in the extractive stage of the extract-then-abstract paradigm in Multi-Document Summarization (MDS). These methods include statistical methods that rely on strict keyword matching and regressive methods that aim to fit predefined metrics like ROUGE. The statistical methods may ignore documents with relevant semantic context, while the regressive methods suffer from data dependence, leading to overfitting or underfitting. These issues result in a gap between the extractive and abstract stages. |
|Solution|To address the semantic discrepancy and data dependence issues, the proposed approach introduces an unsupervised metric for evaluating a document's contextual relatedness with keywords in an unsupervised manner. The approach leverages Pre-trained Language Models (PLMs) and utilizes their ability to calculate the perplexity of a sequence. Unsupervised prompts are constructed by joining keywords with documents to form new sequences, and the perplexity of these sequences represents the semantic salience of the documents. The proposed approach is called Unsupervised Prompt-based ExtractoR (UPER) and can be used as a plug-in to evaluate documents on different datasets or enhance other metrics by combining scores. |
|Preliminary Experiment|Semantic Discrepancy: The preliminary experiment reveals that there is a semantic discrepancy between the unsupervised metric used in the proposed approach (UPER) and the ROUGE scores obtained in the abstractive stage. This discrepancy is observed from the comparison of the 9 boxes in the upper right or lower left part of the matrix. It suggests that the ROUGE score, commonly used as the training objective in previous regressive methods, may narrowly model the objective function of the extractive stage, P(D' | K, D). <br /> 2. Data Dependence: The experiment highlights the data dependence issue of the regressive methods. The fitting results of these methods heavily rely on the training data, which can lead to overfitting or underfitting. This observation supports the motivation of the proposed unsupervised approach (UPER), which aims to overcome the limitations of data-dependent methods.<br /> 3. UPER as a Plug-in: The experiment demonstrates that the unsupervised prompt-based approach, UPER, can be used as a plug-in to evaluate documents on different datasets or enhance other metrics by combining scores. This indicates the versatility and potential applicability of UPER in improving document evaluation and subsequent abstractive stage performance in multi-document summarization tasks.|
| Method | 1. Objective Function Modeling: The paper addresses the challenge of modeling the objective function P(Dâ€² | K, D), which evaluates each document's relevance to the keywords. The objective is to find a proper metric that is unsupervised, models both lexical and semantic salience, and effectively improves the abstractive stage of multi-document summarization. <br /> 2. Perplexity as a Metric: Inspired by recent success with Pre-trained Language Models (PLMs), the proposed method utilizes perplexity, calculated by PLMs, to model the semantic feature of the documents. Perplexity is a metric commonly used in NLP to evaluate the likelihood of a word sequence. It captures the ability of PLMs to encode and generate coherent sequences. By using perplexity, the metric can be applied to any dataset without requiring training. <br />3. Filtering Noisy Documents: To filter out noisy documents that may have keyword overlap but lack effective information, the method aims to retain documents with relevant semantic content even if they don't contain the keyword. This filtering process helps improve the quality of the selected documents for the abstractive stage.<br /> 4. Prompt Design: Prompts have shown effectiveness in modeling contextual relatedness. The method introduces Unsupervised Prompt-based ExtractoR (UPER), which utilizes several prompting templates filled with the keyword and document. This document is about {k}.{d}. These templates, such as putting the keyword in the introduction or conclusion positions, test the contextual appearance of the keyword within the document. The perplexity of the prompt-filled sequence represents the probability of the keyword appearing in the document's context. <br /> 5. Semantic Salience Score: The semantic salience score gS(K, d) of a candidate document is calculated based on the perplexity of the prompt-filled sequences. It measures the probability of the keyword showing up in the context of the document, considering all the designed prompting templates. The score provides an assessment of the document's semantic salience and its relevance to the keywords. |