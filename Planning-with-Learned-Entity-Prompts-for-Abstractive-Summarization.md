# `Planning with Learned Entity Prompts for Abstractive Summarization`


| Topic | Planning with Learned Entity Prompts for Abstractive Summarization|
|-----|--------------------------|
| Problems | Generating abstractive summaries that are faithful, coherent, and specific to entities mentioned in the input document is a challenging task for natural language generation models. Existing approaches that rely on sequence-to-sequence models often suffer from issues such as hallucinations and lack of entity-specificity, which can reduce the quality of the generated summaries. |
| Solution | To address these issues, the authors propose a simple yet flexible mechanism to learn an intermediate plan to ground the generation of abstractive summaries. The approach involves appending the target summary with ordered sequences of entities mentioned in the summary, known as entity chains. Transformer-based sequence-to-sequence models are then trained to generate the entity chain and continue generating the summary conditioned on the entity chain and the input. The authors experiment with both pretraining and finetuning with this content planning objective, achieving state-of-the-art performance on XSum and SAMSum in terms of ROUGE. Moreover, by prompting the decoder with a modified content plan that drops hallucinated entities, they outperform state-of-the-art approaches for faithfulness when evaluated automatically and by humans. |
| Introduction | 1.The traditional text summarization process involves building a source representation, learning a summary representation, and synthesizing the output summary text. Common methods involve semantically analyzing the source text, refining the input representation, and grounding the summary generation to the intermediate summary representation. <br /> 2. State-of-the-art neural summarizers use powerful sequence-to-sequence architectures with attention and copy mechanisms, transformer architectures with multi-headed self-attention, and large pretrained conditional language models. However, grounding the summary generation to an intermediate representation has not been achieved in neural summarization, leading to undesired hallucinations in generated summaries. <br /> 3. The paper investigates using entity chains, ordered sequences of entities in the summary, as an intermediate summary representation to better plan and ground the generation of abstractive summaries. The approach involves using transformer-based encoder-decoder models to generate both the entity chain and the summary conditioned on the entity chain and the input. The results show that the approach outperforms regular finetuning in terms of entity specificity and planning in generated summaries on four popular summarization datasets. The approach can also be used for pretraining summarization models and mitigating hallucinations in abstractive summaries by modifying the predicted entity chain to only keep entities seen in the document. <br />|
| Main Contribution | <strong>1. Planned and Grounded Abstractive Summarization:</strong><br /> The FROST objective is introduced as a novel training objective to neural summarization models for content planning with entity chains, and it aims to ground the generation of summaries to the entity chains found in the reference summaries during training. The objective is designed to try to "freeze entity-level information in abstractive summarization with planning," hence the name FROST. The objective is integrated with supervised fine-tuning and self-supervised pre-training objectives without altering the models themselves. <br /><strong >2. Controlled Abstractive Summarization with Entity Chains:</strong><br />It describes the effectiveness of FROST in entity-level content modification for abstractive summaries. Specifically, the authors show how FROST is critical for ensuring faithfulness by enabling the drop-prompt mechanism, which involves dropping out hallucinated entities from the predicted content plan and prompting the decoder with this modified plan to generate faithful summaries. Additionally, the authors demonstrate that FROST enables the generation of summaries with topical and style diversity, by choosing different sets of entities from the source to plan what to discuss in the summary and by reordering entities in the predicted plan to get an equivalent summary with a different entity emphasis. These findings highlight the importance of FROST in enabling more accurate and diverse summarization.|
| Related Work | <strong>1. Content Planning for Summarization: </strong> <br /> It discusses the different approaches to content planning in text summarization. Traditional methods focused on the granularity of linguistic, domain, and communicative information included in the source representation. Some argued for deep semantic analysis of the source text, while others used shallow semantic analysis using only word frequency or lexical chains. More recent encoder-decoder models tend to perform text generation in an end-to-end setting without explicitly modeling content planning. Some work in data-to-text and story generation explored content planning using templates or sequences of words/events, but no similar work exists for summarization using encoder-decoder models. <br /><strong>2. Pretraining for Summarization and Planning : </strong><br /> It discusses the use of transformer-based models in text generation and how most of these models use task-agnostic pretraining objectives such as language modeling or sequence reconstruction. However, there have been few attempts to use task-specific pretraining for summarization. Some previous attempts include using sentence selection or question generation pretraining. The passage highlights that the proposed solution in the paper, which incorporates content planning directly into pretraining, is novel and to the best of the authors' knowledge, the first of its kind.<br/><strong>3.Controlled Abstractive Summarization:</strong><br />The authors point out that there is a growing interest in controlled abstractive summarization, which aims to enable users to specify high-level characteristics such as length, keywords, and topic to generate summaries that better suit their needs. Current approaches for controlled summarization usually rely on either encoding these features along with the input or using them to filter beams for lexically constrained decoding. In contrast, their approach is more generic as they do not rely on external systems or data to augment the input; instead, users can prompt the decoder with a desired content plan in the form of an entity chain to control the summary.|
| Methodology | <strong>1. Model Formulation:</strong><br /> Here, 'd' be an input document, here our target is to teach model to first generate a content plan 'c' for a summary 's' as p(c/d) and then generate p(s/c,d). We define the ordered chain of entities ovserved in the summary 's' as its content plan. we train an encoder-decorder model to encode the document 'd' and generate the concatenated content plan and summary. Here, decoder first predict the entity chain 'c' and then continures predicting the summary 's'. we predic 'c' and 's' with special markers [ENTITYCHAIN] and [SUMMARY]. The model is trained with the standard maximum-likelihood objective generating the augmented target.<br />
<strong>2. Pretraining Content Plans</strong> <br />![alt text](https://github.com/ayushkoirala/Master-Thesis-Paper-Reading/blob/main/image/peagasus.png)<br />The authors modified PEGASUS, a state-of-the-art transformer-based model for abstractive summarization, to incorporate their approach for entity-level content planning. Specifically, they used a dynamic approach to prepend a selected set of important sentences from an input document with their corresponding entity chain to create a target for pretraining their model. This modified model was then trained to generate this target from the rest of the document.<br /><strong>4. Modeling Entity-Level Lexical Cohesion and Coherence :</strong><br /> Here, we enforce our model to learn entity-level lexucal cohesion and conherence in the summary. so, that c as ( p(c/d) in FROST) will be less susceptible  when predicting them directly in the summary 's' p(s/d). <br /><strong>Controlled Generation with Entity Prompts:</strong> since, we are using same decoder to generate the plan 'c' and summary 's'. so, during the inference time the decoder can be easily prompted with any desired content plan c' to control output summary s'. p(s'/c',d)|
| Experiment | <strong >4.1 Base and Large model:</strong><br /> Base model L=12,H=768,F=3072, A=12(223 M parameter) and Large model L=16,H=1024,F=4096 and A=16(568 M parameters) , Batch_size = 1024, pretrained learning_rate = 0.01 and finetuning =0.0001. finetuned model decoded with beam size of 8.|
| Datasets | <strong>Pretraining Dataset: </strong><br />C4 (350 M web-pages obtained from common crawl) and HugeNews(1.5B news from 2013-2019).<br /><strong>Abstractive Summarization Datasets:</strong> <br />1. CNN/DailyMail highlight generation. <br />2. XSum extreme summarization <br /> 3.  SAMSum dialogue summarization <br /> 4.BillSum summarizing US Congressional bills <br />[alt text](https://github.com/ayushkoirala/Master-Thesis-Paper-Reading/blob/main/image/summarize-dataset.png) <br /> <strong>Entity Chain Annotation :</strong><br />We annotate the whole document in the pretraining datasets to allow dynamic construction of summaries and their entity chains during pretraining. We only annotate the reference summaries for the finetuning summarization datasets. We use a BERT-based tagger trained on CoNLL-2003 data to identify named entities, and regular expressions to identify dates and numbers |
| Evaluation Measures | Evaluated FROST models on ROUGE, entity specificity, entity planning, faithfulness using automatic and human evaluations, and overall quality by humans.|