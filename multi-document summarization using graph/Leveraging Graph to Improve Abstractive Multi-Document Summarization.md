#1. Leveraging Graph to Improve Abstractive Multi-Document Summarization

#2. Paper Link : https://arxiv.org/pdf/2005.10043

| Topic | Leveraging Graph to Improve Abstractive Multi-Document Summarization |
| ---------------| --------------------------- |
| Problem | The paper addresses the challenges of Multi-document Summarization (MDS), which is a complex process due to the need to effectively represent multiple documents and manage long summaries. MDS involves summarizing information from multiple documents that may have overlapping, complementary, or contradictory content, making it difficult to extract salient information, detect redundancy, and generate coherent summaries. |
|Solution Approach |  <strong>1. Graph Representation:</strong> </br> The proposed model leverages explicit graph representations of documents, such as similarity graphs and discourse graphs, to better encode cross-document relations and guide the summary generation process.</br> <strong> 2. Graph-Informed Attention Mechanisms:</strong><br /> These mechanisms help integrate the graph structures into both document encoding and summary generation, enhancing the model's ability to process and summarize complex document sets.</br> <strong> 3. Integration with Pre-trained Language Models:</br ></strong> The model combines the advantages of graph-based modeling with powerful pre-trained language models like BERT, RoBERTa, and XLNet, to handle longer inputs and improve summarization quality.|
|Why Graph? | Represent complex relationships and structures between different text units, which helps in understanding the context and relevance of information across documents. <br/> Enable the model to detect and synthesize the most relevant and salient information, which is critical for producing informative, concise, and coherent summaries.<br /> Offer a structured way to encode and decode information, making it easier to manage the complexity of multiple documents and enhance the coherence of the generated summaries. |
|Methodology|  <div align="right"> <img src="https://github.com/ayushkoirala/Master-Thesis-Paper-Reading/blob/main/image/graph1.png" width="600"> </div> <br /> <strong>1. Document Preparation and Graph Representation</strong><br /> <strong>1. Paragraph Segmentation:</strong> The source documents are segmented into paragraphs by identifying line breaks. This segmentation aids in managing and processing the documents, transforming long text into a series of shorter, more manageable units. <br /> <strong>2. Graph Construction:</strong> For each set of segmented documents, a similarity graph is constructed. This graph represents paragraphs as nodes. Edges between these nodes are weighted by cosine similarities between their respective tf-idf vector representations. This step is critical as it quantifies the relatedness of different paragraphs across the document set, which is foundational for the subsequent graph-based attention mechanisms. <br />  <strong> Encoder-Decoder Framework </strong> This framework is split into two primary components: the encoder and the decoder, both of which utilize layers designed for processing both the textual content and the graph-based relationships. <br /><strong>Encoder</strong> Transformer Layers: These are token-level transformers that process the text within each paragraph. They are designed to capture the intra-paragraph contextual nuances by applying self-attention mechanisms across the tokens within a paragraph. These layers help in understanding the local context and nuances within each paragraph. <strong> Graph Encoding Layers:</strong><br /> These layers extend the traditional transformer architecture by integrating a graph attention mechanism. This mechanism utilizes the graph representation of the document (created in the earlier step) to bring in an additional layer of contextual information that spans across paragraphs (inter-paragraph context). This is achieved through: <br /> <strong>Graph-informed Self-Attention: </strong>It modifies the standard self-attention by integrating a graph-based component. The attention mechanism here is not solely based on the content but is informed by the graph structure, enabling the model to consider the relationships (as defined by the similarity graph) between different paragraphs during the encoding process.|
