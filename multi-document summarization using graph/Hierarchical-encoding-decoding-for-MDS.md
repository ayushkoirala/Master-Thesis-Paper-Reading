#1. A Hierarchical Encoding-Decoding Scheme for Abstractive Multi-document Summarizationn

#2. Paper Link : https://arxiv.org/pdf/2305.08503

| Topic | A Hierarchical Encoding-Decoding Scheme for Abstractive Multi-document Summarizationn |
| ---------------| --------------------------- |
| Problem | The main challenge identified is that while pre-trained language models (PLMs) have shown significant success in single-document summarization, their effectiveness doesn't fully translate to multi-document summarization (MDS). This is primarily due to the complexity of handling cross-document relationships, which is not effectively managed by simply applying PLMs or designing new MDS-specific architectures. |
| Solution | The proposed solution introduces a hierarchical encoding-decoding scheme to enhance the use of PLMs for MDS. This approach maintains the original attention mechanism within individual documents while restructuring inter-document interactions at a higher hierarchical level. This reconfiguration allows for more effective management of cross-document information and leverages the existing pre-trained capabilities of the models for better summarization outcomes. | 
| dataset used | 10 MDS news,  scientific literature, movie reviews, peer reviews, and Wikipedia topics |
| Methodology |  The overarching strategy is to equip PLMs with an enhanced capability for handling the intricacies of MDS while leveraging their inherent strengths. The method involves two primary modifications: <br /> 1. Preserving token interactions within the same document in both the encoder and decoder to maintain context integrity and prevent information leakage between documents. <br /> 2. Using the < s > token for hierarchical document representation in the encoder, and leveraging it for hierarchical attention scaling in the decoder.<br /> <strong>  Hierarchical Encoding Scheme  </strong>  <br /> <strong>>1.  Leveraging PLM Knowledge </strong> <br /> a. Restricted Intra-Document Full Attention: <br /> This adjustment confines the self-attention mechanism to tokens within the same document. It prevents the model from accessing tokens outside the given document, aligning with the training behavior of PLMs and avoiding the misinterpretation of context when dealing with multiple documents. <br /> b. Position Restart: <br /> A positional reset is implemented at the start of each new document within the encoder. This signals to the model that subsequent tokens belong to a new document. This helps maintain the coherence and distinctiveness of individual documents, ensuring that document boundaries are respected. <br /> <strong> Handling Cross-Document Information </strong> To effectively process information across documents, the model incorporates Start-of-Document (SOD) tokens, which are strategically employed to represent and manage document-level information: <br /> a. SOD Representation <br /> Unlike traditional approaches that may use global attention, this method uses the < s > token at the beginning of each document. These tokens are configured to focus on their respective documents while allowing controlled interactions with other SOD tokens from different documents. This setup helps in capturing high-level representations of each document’s context. <br /> b. Document-Level Attention: The SOD tokens are allowed to interact with each other across documents, enabling the model to synthesize information from multiple sources effectively. This attention mechanism is structured to maintain document-specific focus while facilitating broader document comparisons and integrations. |
|Methodology| <strong>Hierarchical Decoding Scheme</strong> In the decoding phase, the approach builds on the encoded representations to generate a coherent and comprehensive summary:<br/> <strong> a. Cross-Attention Mechanism:</strong> The decoder employs a hierarchical structure where the cross-attention weights are influenced by the SOD tokens. These tokens help determine the relative importance of each document’s information, guiding the decoding process to focus more on significant or relevant content. <br /> </strong>Attention Weight Scaling:</strong> The cross-attention scores for each document are scaled based on the significance assigned by the SOD tokens, ensuring that the summary generation process considers the relative importance of the information presented in each document.
