#1. A Hierarchical Encoding-Decoding Scheme for Abstractive Multi-document Summarizationn

#2. Paper Link : https://arxiv.org/pdf/2305.08503

| Topic | A Hierarchical Encoding-Decoding Scheme for Abstractive Multi-document Summarizationn |
| ---------------| --------------------------- |
| Problem | The main challenge identified is that while pre-trained language models (PLMs) have shown significant success in single-document summarization, their effectiveness doesn't fully translate to multi-document summarization (MDS). This is primarily due to the complexity of handling cross-document relationships, which is not effectively managed by simply applying PLMs or designing new MDS-specific architectures. |
| Solution | The proposed solution introduces a hierarchical encoding-decoding scheme to enhance the use of PLMs for MDS. This approach maintains the original attention mechanism within individual documents while restructuring inter-document interactions at a higher hierarchical level. This reconfiguration allows for more effective management of cross-document information and leverages the existing pre-trained capabilities of the models for better summarization outcomes. | 
| dataset used | 10 MDS news,  scientific literature, movie reviews, peer reviews, and Wikipedia topics |
| Methodology |  The overarching strategy is to equip PLMs with an enhanced capability for handling the intricacies of MDS while leveraging their inherent strengths. The method involves two primary modifications: <br /> 1. Preserving token interactions within the same document in both the encoder and decoder to maintain context integrity and prevent information leakage between documents. <br /> 2. Using the “<s>” token for hierarchical document representation in the encoder, and leveraging it for hierarchical attention scaling in the decoder.<br /> <strong>  Hierarchical Encoding Scheme  </strong>  <br /> 1.  Leveraging PLM Knowledge <br /> a. Restricted Intra-Document Full Attention: <br /> This adjustment confines the self-attention mechanism to tokens within the same document. It prevents the model from accessing tokens outside the given document, aligning with the training behavior of PLMs and avoiding the misinterpretation of context when dealing with multiple documents. <br /> b. Position Restart: <br /> A positional reset is implemented at the start of each new document within the encoder. This signals to the model that subsequent tokens belong to a new document. This helps maintain the coherence and distinctiveness of individual documents, ensuring that document boundaries are respected. |