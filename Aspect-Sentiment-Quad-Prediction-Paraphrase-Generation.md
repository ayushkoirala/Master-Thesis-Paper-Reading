Aspect Sentiment Quad Prediction as Paraphrase Generation.
Paper Link : https://arxiv.org/pdf/2110.00796.pdf

| Topic | Aspect Sentiment Quad Prediction as Paraphrase Generation |
| ---------------| --------------------------- |
| Problem | The problem addressed in the paper is that existing approaches for aspect-based sentiment analysis (ABSA) only consider the detection of partial sentiment elements (e.g. aspect category or sentiment polarity)  instead of predicting all four elements (aspect category, aspect term, opinion term, and sentiment polarity) in a comprehensive and complete manner. The authors introduce a new task called Aspect Sentiment Quad Prediction (ASQP) which aims to jointly detect all four sentiment elements in a given opinionated sentence. |
| Solution | The solution proposed in the paper is a novel PARAPHRASE modeling paradigm to cast the aspect-sentiment quad prediction (ASQP) task as a paraphrase generation process. This approach solves the ASQP task in an end-to-end manner, eliminating potential error propagation in a pipeline solution. The PARAPHRASE modeling framework also allows for the full exploitation of the semantics of the sentiment elements by learning to generate them in natural language form. The authors conduct extensive experiments on benchmark datasets and show that their proposed method outperforms existing methods and has the capacity for cross-task transfer. |
| Introduction | 1. Aspect-based sentiment analysis (ABSA) is a fine-grained opinion mining problem that aims to analyze sentiment information at the aspect level. <br /> 2. Four fundamental sentiment elements are involved in ABSA: aspect category (the type of the concerned aspect), aspect term (can be explicitly or implicitly mentioned in the text), opinion term (describes the opinion towards the aspect), and sentiment polarity (sentiment class). Given an example sentence “The pasta is over-cooked!”, the sentiment elements are “food quality”, “pasta”, “overcooked”, and “negative”, respectively. <br /> 3. Many research efforts have been made on ABSA to predict or extract the sentiment elements, with early studies focusing on the prediction of a single element and more recent works proposing to extract multiple associated sentiment elements at the same time. <br /> 4. Despite their popularity, these ABSA tasks only attempt to perform partial prediction instead of providing a complete aspect-level sentiment picture, i.e., identifying the four sentiment elements in one shot. <br /> 5. The paper introduces the aspect sentiment quad prediction (ASQP) task, which aims to predict all four components (aspect category, aspect term, opinion term, sentiment polarity) for a given opinionated sentence. It proposes to tackle the ASQP task using a sequence-to-sequence (S2S) approach instead of the traditional pipeline approach. This approach eliminates potential error propagation and fully exploits the rich label semantic information by learning to generate the sentiment elements in natural language form. <br /> 6. The authors propose a novel PARAPHRASE modeling paradigm to tackle two challenges in ASQP task: linearizing the sentiment information for S2S learning and utilizing pretrained models. The paradigm transforms the ASQP task into a paraphrase generation problem where the sentiment quad is linearized into a natural language sentence. This allows the use of large pretrained generative models such as T5 by fine-tuning with the input-target pairs. The rich label semantics and pretrained knowledge are combined in the form of natural sentences. |
| Contribution | 1. They introduced a new task called aspect sentiment quad prediction (ASQP) and created two datasets with sentiment quad annotations to analyze comprehensive aspect-level sentiment information. <br /> 2.  They proposed to tackle ASQP as a paraphrase generation problem, which can predict sentiment quads in one shot and take advantage of the natural language label semantics. <br /> 3. Experiments showed that their proposed PARAPHRASE modeling approach outperforms previous state-of-the-art models in ABSA tasks. <br /> 4 . The experiments also suggest that the PARAPHRASE method enables knowledge transfer across related tasks and can be particularly useful in low-resource settings. |
| Methodology |<strong>3.1 Problem Statement:</strong> ASQP is a task that predicts aspect-level sentiment quadruplets in a sentence. This quadruplet consists of the aspect category (c), aspect term (a), opinion term (o), and sentiment polarity (p). The aspect category belongs to a predefined category set (Vc), while the aspect and opinion terms are text spans within the sentence (a ∈ Vx ∪ {∅} and o ∈ Vx). The sentiment polarity can be positive (POS), neutral (NEU), or negative (NEG). <br /><strong>3.2 ASQP as a Paraphrase Generation:</strong> The authors propose a new approach, called PARAPHRASE modeling, to tackle the ASQP task. This approach transforms ASQP into a paraphrase generation problem and uses an <strong>encoder-decoder model (M)</strong> to generate a target sequence (y) from the input sentence (x). The target sequence contains all the necessary sentiment elements, and the sentiment quadruplets (Q) can be recovered from this target sequence to make the final prediction. The advantage of this approach is that it can fully exploit the semantics of the sentiment elements in the target sequence and utilize the rich knowledge in the pretrained generative model as both the input and target are natural language sentences. <br />![alt text](https://github.com/ayushkoirala/Master-Thesis-Paper-Reading/blob/main/image/image-4.png) <br /> <strong> Paraphase Modeling :</strong>The PARAPHRASE modeling framework uses a sequence-to-sequence (S2S) learning approach. In this framework, the sentiment quadruplets (Q) in the input sentence (x) are linearized into a natural language sequence (y) in order to create the input-target pair (x, y). The goal of this linearization process is to highlight the major sentiment elements in the target sentence while ignoring any unnecessary details in the input sentence.  <br /> ![alt text](https://github.com/ayushkoirala/Master-Thesis-Paper-Reading/blob/main/image/image-5.png) To linearize a sentiment quad (c, a, o, p), the authors use a projection function (Pz) for each sentiment element (z), where z can be any of the elements in the sentiment quad (c, a, o, p). The projection function maps each sentiment element from its original format to a natural language form. By using suitable projection functions, a structured sentiment quad can be transformed into an equivalent natural language sentence. <br />For an input sentence with multiple sentiment quads, each quad is first linearized into a natural sentence. Then, these sentences are combined with a special symbol [SSEP] to form the final target sequence y, which contains all the sentiment quads for the given sentence. <br /> <strong>Target Construction for ASQP :</strong> <br/>1. The sentiment quad consists of 4 elements: aspect category (c), aspect term (a), opinion term (o), and sentiment polarity (p).The aspect category and opinion term are already in natural language form, so their projection functions just keep the original formats (Pc(c) = c and Po(o) = o).<br />2. The sentiment polarity is transformed from the original class format to a natural language expression (great for POS, ok for NEU, and bad for NEG). This transformation ensures the coherence of the target sentence and allows the generation model to exploit the sentiment polarity semantics.<br />3.The aspect term is mapped to an implicit pronoun (it) if it is not explicitly mentioned, otherwise the original natural language form is used (Pa(a) = it if a = ∅, a otherwise). This mimics the writing process where users often use pronouns to refer to an unexpressed target.<br />3. After defining the specific projection functions, the sentiment quad can be transformed into a sentence containing all the elements in natural language form. This facilitates the S2S learning process.<br /> <strong>3.3 Sequence-to-Sequence Learning: </strong> <br />1.  The input-to-target generation is modeled using an encoder-decoder model, such as the Transformer architecture. Here, Given sentence x, the encoder first transforms it into a contexutalized encoded sequence e. The decoder then models the conditional probability distribution of the target sentence y given the encoded input representation (pθ(y/e)).<br /> 2. At each time step, the decoder output (yi) is computed based on both the encoded input (e) and previous outputs (y < i).<br />3. The decoder uses a softmax function to compute the probability distribution for the next token, mapping the prediction (yi) to a logit vector and computing the probability distribution over the whole vocabulary set.|
