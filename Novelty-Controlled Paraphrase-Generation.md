# `Novelty Controlled Paraphrase Generation with Retrieval Augmented Conditional Prompt Tuning`


| 1. Topic | Novelty Controlled Paraphrase Generation with Retrieval Augmented Conditional Prompt Tuning|
|-----|--------------------------|
| Problem | The task of paraphrase generation involves rephrasing a given text while retaining its meaning, and it has several applications including text simplification, semantic parsing, query re-writing, and data augmentation. However, the task can be challenging, and the level of lexical novelty of generated paraphrases is usually left upon the model to decide, which may result in trivial paraphrases that are not helpful for applications such as data augmentation.|
| Solution | To address these challenges, the authors propose two methods. <br />1. First, they introduce a method called <strong>Retrieval Augmented Prompt Tuning (RAPT)</strong>, which augments prompt tuning with kNN-based retrieved examples. RAPT can enhance the quality of paraphrases while reducing the number of trainable parameters, making it more parameter-efficient than the standard method of fine-tuning all parameters. <br />2.The authors propose a model-agnostic method called <strong>Novelty Conditioned RAPT (NC-RAPT)</strong>, which uses specialized prompts for different levels of novelty to enable novelty-controlled paraphrase generation. NC-RAPT gives users more control over the level of novelty they want in the generated paraphrases, which can generate paraphrases that are more useful for specific applications, such as data augmentation. Overall, these approaches can improve the quality and usefulness of generated paraphrases, which can benefit various natural language processing tasks.|
| Task Definition | 1. Paraphrase generation is a sequence-to-sequence problem: In paraphrase generation, we aim to generate a new sequence of tokens that is a rephrased version of the input sequence X<sub>1:n </sub>=(x1,x2,x3....x<sub>n</sub>) and output or another sequence is  y<sub>1:m</sub>=(y1,y2.....y<sub>m</sub>). In the experiment, They have used GPT-based models, which are pre-trained on autoregressive language modeling or auto-completion, are used for paraphrase generation. This approach allows the downstream task to remain similar to the pre-training task, making the transfer of pre-trained knowledge easier.  So, our <strong>Input </strong>is  <strong>prompt prefix</strong>,“\n Paraphrase: ” is the <strong>prompt infix</strong>, “x1, x2, x3, . . . , xn” is the input. and <strong>output</strong> is “y1, y2, y3, . . . , ym” is the paraphrased output. So, t. The prefix and infix together forms the prompt template:“Input: \n Paraphrase: ”. <br />2.We can further generalize the input prompt format by formalizing a generic sequence of tokens denoting the prompt prefix as p1:s = (p1, p2, p3, . . . , ps) and a generic sequence of tokens denoting the prompt infix as q1:t = (q1, q2, q3, . . . , qt). Overall the generalized input prompt will be: “p1, p2, p3, . . . , ps, x1, x2, x3, . . . , xn, q1, q2, q3, . . . , qt”.|
| Baseline |<strong>1.Fine Tuning :</strong> <br />Fine-tuning involves updating the pre-trained parameters of a model using gradient descent in the context of a downstream task. In this case, the downstream task is paraphrase generation, and the pre-trained model is adapted by updating its parameters based on how well it performs on this task.<br /> The implementation of fine-tuning used in this case involves manually designing an input prompt that includes the input sequence (x1:n) and a placeholder for the generated paraphrase. The model is trained using an auto-completion framework, where it is given the input sequence and asked to generate a completion that is a paraphrase of the input. The model is then fine-tuned by updating its parameters to minimize the difference between its generated paraphrase and a target paraphrase.<br /><strong>2. Adapter Tuning(AT):</strong><br /> a. Adapter Tuning (AT) is a method for adapting a pre-trained Transformer model by introducing low-rank bottlenecks in the form of adapter layers within each Transformer layer. Unlike fine-tuning, which updates all the parameters of a pre-trained model, AT focuses on tuning only the adapter layer parameters and the layer normalization parameters during training.<br />b. In the implementation used in this case, the same input prompt format and training framework as fine-tuning is used. However, the key difference is that during training, only the adapter layers and layer normalization parameters are updated based on the task-specific loss, while the pre-trained Transformer layers are kept fixed. This can lead to faster training and better performance, particularly when the downstream task is closely related to the pre-training objective of the model.<br /><strong>3. Low Rank Adaptation(LoRA) </strong><br />we can  W<sub>x</sub> a pretrain matrix which is constrains the update of  W<sub>x</sub> through the low-range matrix w<sub>delta</sub>so, w<sub>x</sub>' = Wx + W delta LoRA is applied only to the query and value transformation matrices in the multi-head attention sublayers. The model is trained in the same framework as fine-tuning, but instead of updating all parameters of the pre-trained model, only a much smaller number of parameters involved in the construction of Wδ are updated to adapt the model to the downstream task. This can result in faster training and better performance, particularly when the pre-trained model is already quite effective at the task but requires some fine-tuning.<br /><strong>4.Prompt Tuning :</strong><br /> our input prompt format is: : “p1, p2, p3, . . . , ps, x1, x2, x3, . . . , xn, q1, q2, q3, . . . , qt”.  and The initial representation after passing this input prompt sequence through the embedding layer can be presented as: “e(p1), . . . , e(ps), e(x1), . . . , e(xn), e(q1), . . . , e(qt)”. we directly initialize and tune the prefix (e(p1), . . . , e(ps)) and infix embeddings (e(q1), . . . , e(qt)). When using prompt tuning alone, we only tune the prefix and infix embeddings  while keeping all other parameters frozen <br /><strong>5. LoRA + Prompt Tuning:</strong> Both the prefix-infix parameters and the newly introduced LoRA parameters. Both LoRA and prompt tuning tune only a minor fraction of the total parameters. Thus, we can easily combine them without increasing the overall parameter count significantly.<br/>|
| Approach : | <strong>1. Retrieval Augumented Prompt Tuning(RAPT):</strong><br /> <strong>a. Example-Augumented Prompts:</strong> input text sequence is X= X<sub>1:n</sub> then we want to paraphrase and we have 2 exemplar pair (X1,Y1) and (X2,Y2). so input prompt to a GPT-3 model can be in the form of : <br /> ![alt text](https://github.com/ayushkoirala/Master-Thesis-Paper-Reading/blob/main/image/aug-p1.png)<br /> The authors propose a new format for the input prompt that includes a global prefix d1:m serving as a task description. The prompt is still augmented with task exemplars, but the prefix and infix sequences (p1:s and q1:t) are repeated for each example. The new format is given by:<br /> ![alt text](https://github.com/ayushkoirala/Master-Thesis-Paper-Reading/blob/main/image/aug-p2.png)<br/> The prefix and infix sequences are repeated for each example, which can cause the total length of the input prompt to become very large. To address this issue, the authors revise the prompt design format to include a global prefix <strong>d1:m:</strong> <br /><strong>2. Novelty Conditioned RAPT (NC-RAPT):</strong><br /> 2 major steps : 1st steps :<br/>We classify each sample in dataset according to the novelty of the ground truth paraphrase. It uses Translation Edit Rate(TER) between the input sequence and the ground truth to classify the novalty of the ground truth. 3 classes high, medium, low. <br/> 2nd steps : <br /> ![alt text](https://github.com/ayushkoirala/Master-Thesis-Paper-Reading/blob/main/image/aug-p3.png)|
| Dataset | 4 different dataset: Quora Question Pairs 50K split (QQP 50K),Quora Question Pairs 140K split (QQP 140K),Microsoft Research Paraphrase Corpus (MSRPC),ParaSCI-ACL.|
| Generated Text | ![alt text](https://github.com/ayushkoirala/Master-Thesis-Paper-Reading/blob/main/image/aug-p4.png) |
| Result | ![alt text](https://github.com/ayushkoirala/Master-Thesis-Paper-Reading/blob/main/image/aug-result.png)|