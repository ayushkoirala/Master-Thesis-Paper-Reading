# `Novelty Controlled Paraphrase Generation with Retrieval Augmented Conditional Prompt Tuning`


| 1. Topic | Novelty Controlled Paraphrase Generation with Retrieval Augmented Conditional Prompt Tuning|
|-----|--------------------------|
| Problem | The task of paraphrase generation involves rephrasing a given text while retaining its meaning, and it has several applications including text simplification, semantic parsing, query re-writing, and data augmentation. However, the task can be challenging, and the level of lexical novelty of generated paraphrases is usually left upon the model to decide, which may result in trivial paraphrases that are not helpful for applications such as data augmentation.|
| Solution | To address these challenges, the authors propose two methods. <br />1. First, they introduce a method called <strong>Retrieval Augmented Prompt Tuning (RAPT)</strong>, which augments prompt tuning with kNN-based retrieved examples. RAPT can enhance the quality of paraphrases while reducing the number of trainable parameters, making it more parameter-efficient than the standard method of fine-tuning all parameters. <br />2.The authors propose a model-agnostic method called <strong>Novelty Conditioned RAPT (NC-RAPT)</strong>, which uses specialized prompts for different levels of novelty to enable novelty-controlled paraphrase generation. NC-RAPT gives users more control over the level of novelty they want in the generated paraphrases, which can generate paraphrases that are more useful for specific applications, such as data augmentation. Overall, these approaches can improve the quality and usefulness of generated paraphrases, which can benefit various natural language processing tasks.|
| Task Definition | 1. Paraphrase generation is a sequence-to-sequence problem: In paraphrase generation, we aim to generate a new sequence of tokens that is a rephrased version of the input sequence X<sub>1:n </sub>=(x1,x2,x3....x<sub>n</sub>) and output or another sequence is  y<sub>1:m</sub>=(y1,y2.....y<sub>m</sub>). In the experiment, They have used GPT-based models, which are pre-trained on autoregressive language modeling or auto-completion, are used for paraphrase generation. This approach allows the downstream task to remain similar to the pre-training task, making the transfer of pre-trained knowledge easier.  So, our <strong>Input </strong>is  <strong>prompt prefix</strong>,“\n Paraphrase: ” is the <strong>prompt infix</strong>, “x1, x2, x3, . . . , xn” is the input. and <strong>output</strong> is “y1, y2, y3, . . . , ym” is the paraphrased output. So, t. The prefix and infix together forms the prompt template:“Input: \n Paraphrase: ”. <br />2.We can further generalize the input prompt format by formalizing a generic sequence of tokens denoting the prompt prefix as p1:s = (p1, p2, p3, . . . , ps) and a generic sequence of tokens denoting the prompt infix as q1:t = (q1, q2, q3, . . . , qt). Overall the generalized input prompt will be: “p1, p2, p3, . . . , ps, x1, x2, x3, . . . , xn, q1, q2, q3, . . . , qt”.|
