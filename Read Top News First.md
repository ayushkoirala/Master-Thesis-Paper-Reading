Paper Name : Read Top News First: A Document Reordering Approach for Multi-Document News Summarization

Link :https://arxiv.org/pdf/2203.10254.pdf

|Topic | Read Top News First: A Document Reordering Approach for Multi-Document News Summarization|
|-----|----|
|Problem |The paper addresses the problem of multi-document news extractive summarization (MDS), which involves extracting the most important information from multiple related news documents to create a concise summary. The conventional approach of concatenating documents in an arbitrary order for summarization does not consider the varying importance of different documents. Additionally, many summarization systems tend to prioritize the beginning of the document, which may result in overlooking important information later in the text. Therefore, the paper aims to tackle these issues by proposing a document reordering approach to improve the effectiveness of the summarization model.|
|Solution |The proposed approach focuses on reordering the input documents based on their relative importance before applying a summarization model. The key idea is to prioritize the more salient or detailed information-containing documents and place them at the beginning of the meta-document. This reordering is expected to facilitate the summarization model in learning and summarizing the most significant content more effectively.|
|Method |<strong>1. Supervised Approach:</strong><br />In the supervised approach, a BERT-based model is used to learn the importance scores for each document. The documents are concatenated together, and special tokens ([CLS] and [SEP]) are inserted at the start and end of each document. This concatenated input is encoded using BERT to obtain document representations, denoted as t1, t2, ..., tm. To capture inter-document relationships, a 2-layer Transformer is employed to encode the document representations, resulting in contextualized representations h1, h2, ..., hm. The importance score, ŷi, for the i-th document is predicted using a linear transformation with a softmax function applied to the document's contextualized representation:ŷi = softmax(Whi + b).Here, W and b are learnable parameters. During training, the oracle importance score of each document (denoted as yi) is determined based on the normalized ROUGE-1 F score between the document and the gold abstractive summary. The learning objective is to minimize the Kullback-Leibler divergence between the predicted importance score distribution ŷ and the oracle distribution y.<br /><strong>2. unsupervised approach:</strong><br /> the hypothesis is that the importance of a document is related to its centrality. To compute the centrality of a document di, the topic of the input cluster (denoted as Ti) is represented by concatenating the top-3 sentences from each document except di. The centrality of di is then calculated as the ROUGE score between di and Ti. By choosing the top-3 sentences to represent the topic, the approach aims to prevent the centrality of di from being dominated by its own sentences.|
