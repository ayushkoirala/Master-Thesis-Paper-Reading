# `PSP: Pre-trained Soft Prompts for Few-Shot Abstractive Summarization`


| 1. Topic | PSP: Pre-trained Soft Prompts for Few-Shot Abstractive Summarization|
|---------|--------------------|
| Problem | Few-shot abstractive summarization is a challenging task in natural language generation, and existing methods require tuning a large number of parameters to achieve good results. This is computationally expensive and may not be practical in real-world scenarios.|
| Solution | To address this problem, the authors propose a novel soft prompts architecture coupled with a prompt pre-training plus prompt fine-tuning paradigm. The soft prompts comprise continuous input embeddings across an encoder and a decoder and include a new inner-prompt placed in the text to capture document-level information. <br /> In the training process, the authors first use prompt pre-training with self-supervised pseudo-data to teach the model basic summarizing capability. Then, with few-shot examples, only the designed lightweight soft prompts are fine-tuned. |
| Introduction | Abstractive summarization is a highly demanding and challenging task due to the high labor costs of obtaining quality summaries, which makes few-shot learning important. Fine-tuning large pre-trained language models with few-shot examples often leads to overfitting and is not efficient as it requires updating and storing a large number of pre-trained parameters. <br /> Pre-trained language models such as GPT-3 and prompt learning techniques, like Prompt Tuning and PrefixTuning, have shown promise in few-shot learning, but their performance in abstractive summarization is not satisfactory.<br />The authors have developed a soft prompt tuning method specifically for summarization, which adds prompt tokens before the decoder input tokens to guide the generation process. The method also includes three inner prompts - interval, sequential, and fixed-length - to capture the structure and semantics of the source document and assist in generating document-related content. The method outperforms full-model tuning and naive Prompt Tuning with fewer parameters, and yields a performance competitive to Prefix-Tuning with even fewer trainable parameters. The model is effective and efficient, and the designed prompts in the embedding layer are able to extract knowledge from the encoder language model and guide the decoder language model to generate fluent summaries.|
