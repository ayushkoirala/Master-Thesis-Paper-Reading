# `Quality Controlled Paraphrase Generation`
paper link : https://aclanthology.org/2022.acl-long.45.pdf

| Topic | Quality Controlled Paraphrase Generation |
| ---------------| --------------------------- |
| Problem | Paraphrase generation is important for many downstream tasks, but generating high-quality paraphrases is challenging due to the need to balance semantic similarity and linguistic diversity. Previous methods have focused on controlling specific aspects of the paraphrase, such as its syntactic tree, but have not allowed for direct control of the overall quality of the generated paraphrase. |
| Solution | The authors propose a new model, called QCPG, which allows for direct control of the quality dimensions of the generated paraphrase. They also develop a method for identifying points in the quality control space that are expected to yield optimal generated paraphrases. The authors demonstrate that their approach is able to generate high-quality paraphrases that maintain the original meaning while achieving higher linguistic diversity than uncontrolled baselines.|
| Introduction | 1. QCPG is a Quality Controlled Paraphrase Generation model that takes an input sentence and three quality constraints, represented by a three-dimensional vector of semantic similarity, syntactic and lexical distances, and produces a target sentence that conforms to the specified quality constraints.<br />2. The proposed constraints are simpler than previously suggested ones, such as parse trees or keyword lists, allowing the model the freedom to choose how to attain the desired quality levels. This enables flexibility in generating paraphrases of various flavors and even non-paraphrases that may be useful for downstream tasks. <br />3. Despite the mixed quality of the training data and the scarcity of high-quality data, the QCPG model is able to learn high-quality paraphrasing behavior, increasing the linguistic diversity of the generated paraphrases without decreasing the semantic similarity compared to the uncontrolled baseline. Overall, the QCPG model enables direct control of the three quality dimensions, providing flexibility with respect to the specific requirements of the task at hand and opening a range of generation possibilities. |
| Method | <strong>1. Quantifying Paraphrase Quality:</strong> <br/>- The paper proposes a three-dimensional vector to represent the quality of paraphrases, with the dimensions being semantic similarity, syntactic variation, and lexical variation. i.e. paraphrase quality as a three dimensional vector q(s, s′) =(q<sub>sem</sub>(s, s′), q<sub>syn</sub>(s, s′), q<sub>lex</sub>(s, s′)), where q <sub>sem</sub> is a measure of semantic similarity, and q<sub>syn</sub> and q <sub>lex</sub> are measures of syntactic and lexical variation, respectively.<br/>- The syntactic variation score q<sub>syn</sub>(s, s′)  is based on the normalized tree edit distance between the third level constituency parse trees of the input sentence and the paraphrase, while the lexical variation score q<sub>lex</sub>(s, s′) is based on the normalized character-level minimal edit distance between the bag of words. <br />- The semantic similarity score q<sub>sem</sub>(s, s′) is based on the Bleurt score, which was found to have the highest correlation with human judgments in a study of candidate metrics. All three quality dimensions are normalized to a uniform range of values between 0 and 100.<br/> <strong>2. The QCPG Model:</strong> ![alt text](https://github.com/ayushkoirala/Master-Thesis-Paper-Reading/blob/main/image/QC.png) <br/>- The QCPG model is an encoder-decoder model that is trained on the task of controlled paraphrase generation, with the goal of generating an output paraphrase that conforms to a given control vector c = (csem, csyn, clex).<br />- The QCPG model is trained using the training set pairs (s, t), with the control vector c set to be q(s, t) and maximizing P (t/s,c = q(s,t)) over the training set via the autoregressive cross entropy loss.<br/><strong>3. Control Values Selection</strong><br />- One of the major challenges in controlled paraphrase generation is selecting appropriate input control values that can be achieved by the model. This is because not all paraphrase qualities are achievable for every sentence. Some sentences are harder to paraphrase while maintaining their meaning, which limits the potential lexical diversity of paraphrases involving certain terms like named entities and numbers.<br />- To address this challenge, the authors propose a method for predicting the input control values, c(s), that optimize the expected quality of the paraphrases generated by the quality controlled paraphrase generation model (QCPG). They assume that the quality distribution p(q/s) of all paraphrases of sentence `s` is approximately normally distributed around a sentence dependent mean q<sub>0</sub>(s), and that the variance is approximately sentence-independent.<br />- The authors further assume that given an input sentence `s`, the difficulty to generate a paraphrase of a given quality `q` is dominated by p(q/s) rather than by the quality vector `q` itself. To account for this, they define the input control c(s) for QCPG as the sum of q0(s) and an offset o (O<sub>sem</sub>,O<sub>syn</sub>,O<sub>lex</sub>), which represents the level of difficulty in generating a paraphrase of a particular quality. They then analyze the model results for varying levels of difficulty by varying the offset o from q<sub>0</sub>(s)<br /><strong>4. The Quality Predictor (QP): </strong><br/>- The reference point r(s) plays a crucial role in our solution as it provides a measure of the expected quality of the paraphrases that can be generated for a given input sentence `s`. By using a regressor model to approximate q<sub>0</sub>(s), we are able to estimate the optimal control values that can be used by QCPG to generate high-quality paraphrases that conform to the desired control values.<br />- <strong>In Summary:</strong> decomposing the input control into a sentence dependent reference point r(s) and a sentence independent offset, O, we are able to optimize the expected quality of the paraphrases generated by QCPG while taking into account the inherent difficulty of paraphrasing certain types of sentences. |
| Dataset and Implementation | 1. MSCOCO : 123K where each image contains at most five human-labeled captions.<br />2. WikiAnswer : There are 30,370,994 cluster with 25 question in each on average. In total corpus contain 70 Million question pair <br /> ParaBank 2.0 : clusters of sentential paraphrases, produced from a bilingual corpus using negative constraints, inference sampling, and clustering. The dataset is composed of avarage of 5 paraphrases in every
cluster and close to 100 million pairs in total <br /> <strong>Implementation Details: </strong><br />- All models are trained with batch size of 32 on 2 NVIDIA A100 GPUs for 6 epochs. <br />- The pre-trained T5-base model is utilized as the encoder-decoder model, with the control input vector being quantized into 20 equally spaced values ranging from 0 to 100. These values are assigned to special saved-tokens, which are concatenated to the head of the input sentence and used as input to the model. Additionally, the sentence-dependent reference point (r(s)) and sentence-independent offset (o) are also quantized in a similar manner. |
| Results | ![alt text](https://github.com/ayushkoirala/Master-Thesis-Paper-Reading/blob/main/image/Qc-result.png) |
