`Topic:  "Peek Across: Improving Multi-Document Modeling via Cross-Document Question-Answering" `
Link: https://arxiv.org/pdf/2305.15387.pdf

|Topic||
|----|----|
|Problem|The paper addresses the problem of effectively modeling and understanding information from multiple documents in the context of multi-document tasks. While existing approaches have shown improvements by pre-training language models on multi-document objectives, there is still a need for a more comprehensive approach that leverages cross-document information and covers a wider range of tasks, including both short text generation (e.g., question answering) and long text generation (e.g., summarization).|
|Solution|The proposed solution, called QAMDEN (Question Answering Model with Document-level Encoding and pre-training), introduces a novel cross-document question answering pre-training objective. The approach involves the following steps:<br />1.Generating Semantically-Oriented Questions: Given a set of topically-related documents, the model systematically generates semantically-oriented questions from a salient sentence in one document. These questions serve as a way to challenge the model during pre-training.<br />2.Peeking Across Documents:During pre-training, the model is tasked with answering the generated questions while "peeking" into other topically-related documents. This cross-document perspective allows the model to better understand and recover cross-text informational relations. <br />Sentence Recovery:Additionally, the model is challenged to recover the sentence from which the question was generated, again leveraging cross-document information. This task further enhances the model's ability to capture and utilize document-level context |
|Introduction|<strong>1.Increasing attention to multi-document processing:</strong> With the growing volume of textual data and available documents online, there is a need for effective methods to handle and process multiple documents simultaneously. This is particularly relevant for tasks such as multi-document summarization, query-focused multi-document summarization, and multi-hop question answering.<br />2. <strong>Challenges in multi-text processing: </strong>Existing NLP models are primarily designed to handle single texts, making it challenging to process multiple documents together. Early solutions for multi-text processing were task-specific and lacked generalization across different multi-document tasks.<br /><strong>3.Efficient language models for multi-document processing:</strong> Recent developments in efficient language models demonstrated that by concatenating multiple documents into a single sequence, the transformer-based models can identify and connect relevant information across the documents. Pre-training these models on multi-document objectives improves their performance on downstream tasks.<br /><strong>4. Limitations of existing multi-document pre-training objectives:</strong>Previous pre-training objectives for multi-document models were either limited to classification tasks or focused on summarization. These objectives involved unmasking tokens or generating masked sentences, encouraging the models to recover missing information using other documents.<br /><strong>5.Proposal of a novel pre-training objective:</strong> The paper proposes a novel pre-training objective that supports both short and long text generation, making the multi-document language model more versatile. The objective involves generating questions and answers involving multiple documents, encouraging the model to learn fine-grained information and cross-document relationships.<br /><strong>6.Expansion of pre-training corpora:</strong> The paper introduces a method to expand the pre-training data by generating multiple views for each document cluster. Multiple pairs of questions and answers are generated, conditioned on a subset of the documents in the cluster. This augmentation technique increases the diversity of pre-training data and exposes the model to a variety of contexts.<br /><strong>7.Contributions of the proposed approach: </strong>The paper's contributions include the formulation of a cross-document question answering task as a pre-training approach for multi-document modeling. The method allows the generation of a large number of pre-training examples, not bounded by the number of clusters. The resulting model, called QAMDEN (Question-Answering-based Multi-DocumENt), advances the state-of-the-art for several multi-document tasks.|