#`Topic:  "Peek Across: Improving Multi-Document Modeling via Cross-Document Question-Answering" `
Link: https://arxiv.org/pdf/2305.15387.pdf

|Topic|Peek Across: Improving Multi-Document Modeling via Cross-Document Question-Answering|
|----|----|
|Problem|The paper addresses the problem of effectively modeling and understanding information from multiple documents in the context of multi-document tasks. While existing approaches have shown improvements by pre-training language models on multi-document objectives, there is still a need for a more comprehensive approach that leverages cross-document information and covers a wider range of tasks, including both short text generation (e.g., question answering) and long text generation (e.g., summarization).|
|Solution|The proposed solution, called QAMDEN (Question Answering Model with Document-level Encoding and pre-training), introduces a novel cross-document question answering pre-training objective. The approach involves the following steps:<br />1.Generating Semantically-Oriented Questions: Given a set of topically-related documents, the model systematically generates semantically-oriented questions from a salient sentence in one document. These questions serve as a way to challenge the model during pre-training.<br />2.Peeking Across Documents:During pre-training, the model is tasked with answering the generated questions while "peeking" into other topically-related documents. This cross-document perspective allows the model to better understand and recover cross-text informational relations. <br />Sentence Recovery:Additionally, the model is challenged to recover the sentence from which the question was generated, again leveraging cross-document information. This task further enhances the model's ability to capture and utilize document-level context |
|Introduction|<strong>1.Increasing attention to multi-document processing:</strong> With the growing volume of textual data and available documents online, there is a need for effective methods to handle and process multiple documents simultaneously. This is particularly relevant for tasks such as multi-document summarization, query-focused multi-document summarization, and multi-hop question answering.<br />2. <strong>Challenges in multi-text processing: </strong>Existing NLP models are primarily designed to handle single texts, making it challenging to process multiple documents together. Early solutions for multi-text processing were task-specific and lacked generalization across different multi-document tasks.<br /><strong>3.Efficient language models for multi-document processing:</strong> Recent developments in efficient language models demonstrated that by concatenating multiple documents into a single sequence, the transformer-based models can identify and connect relevant information across the documents. Pre-training these models on multi-document objectives improves their performance on downstream tasks.<br /><strong>4. Limitations of existing multi-document pre-training objectives:</strong>Previous pre-training objectives for multi-document models were either limited to classification tasks or focused on summarization. These objectives involved unmasking tokens or generating masked sentences, encouraging the models to recover missing information using other documents.<br /><strong>5.Proposal of a novel pre-training objective:</strong> The paper proposes a novel pre-training objective that supports both short and long text generation, making the multi-document language model more versatile. The objective involves generating questions and answers involving multiple documents, encouraging the model to learn fine-grained information and cross-document relationships.<br /><strong>6.Expansion of pre-training corpora:</strong> The paper introduces a method to expand the pre-training data by generating multiple views for each document cluster. Multiple pairs of questions and answers are generated, conditioned on a subset of the documents in the cluster. This augmentation technique increases the diversity of pre-training data and exposes the model to a variety of contexts.<br /><strong>7.Contributions of the proposed approach: </strong>The paper's contributions include the formulation of a cross-document question answering task as a pre-training approach for multi-document modeling. The method allows the generation of a large number of pre-training examples, not bounded by the number of clusters. The resulting model, called QAMDEN (Question-Answering-based Multi-DocumENt), advances the state-of-the-art for several multi-document tasks.|
|Methodology|<strong>1.Motivation from previous works</strong><br /> a.Previous studies have shown that pre-training language models (LMs) to generate "summary-like" sequences, called pseudo summaries, improves performance for text summarization tasks.<br /> b. PEGASUS and PRIMERA models were pre-trained using the Gap Sentence Generation (GSG) method, which involves masking highly-ranked salient sentences in a document and tasking the model to generate them.<br /><strong>2.  Cross-Document GSG:</strong>a. The authors propose augmenting the GSG technique to create a cross-document question answering pre-training objective for multi-document tasks.<br />b. Given a cluster of related documents, the cross-document (CD) GSG salience score for a sentence is computed based on its ROUGE score with respect to other sentences within the document and the other documents in the cluster.<br /><strong>3. Generating Cross-Document QAs</strong>a.To generate cross-document questions and answers, the authors employ QASEM, a semantic parsing framework for question generation.<br />b.QASEM labels each verbal predicate-argument relation with a question-answer pair, where the question represents a semantic role and the answer corresponds to an argument in the input text.<br />c.The model from Pyatkin et al. (2021) is used to transform the question into a more natural and clear form, with contextualized arguments.<br />d.The question-answer pair with the longest argument produced by QASEM is selected.<br /><strong>4: Data Generation</strong><br />Three different modes are proposed for constructing a multi-document context:<br />a. Excluding the source document: The most salient sentence in the held-out document is used to generate a QA pair, and the model predicts the answer without access to the source document.<br />b) Masking the salient sentence: The source salient sentence is masked, and the model has access to the surrounding context in the held-out document and other documents in the set.<br />c) Masking the answer: Only the answer span within the salient sentence is masked, and the model has access to the surrounding salient sentence and all the documents in the set.<br /><strong>Appending question and generating output:</strong><br />a.As part of the pre-training process, the question is appended after the context, and the model is instructed to generate an answer followed by its salient sentence.<br />b.This setup introduces a copying mechanism, allowing the model to learn to copy information from the source directly, and enables long-text generation, which is crucial for summarization tasks.