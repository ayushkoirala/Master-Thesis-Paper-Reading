# `LBMT team at VLSP2022-Abmusu: Hybrid method with text correlation and generative models for Vietnamese multi-document summarization`
paper link : https://arxiv.org/pdf/2304.05205.pdf

| Topic | Hybrid method with text correlation and generative models for Vietnamese multi-document summarization |
| ---------------| --------------------------- |
| Problem Statement | Multi-document summarization is a challenging task, especially when dealing with the Vietnamese language, that requires the selection of the most important information from multiple documents to create a coherent summary. Extractive and abstractive models have been proposed for this task, but abstractive models are more difficult to train as they require a deep understanding of the input documents. Furthermore, the development of pretrained language models for Vietnamese presents new opportunities for improving multi-document summarization performance. |
| Solution Apprach | In this paper, the authors propose a pipeline abstractive method for multi-document summarization in Vietnamese. The method consists of three main phases. In the first phase, single document extraction is performed to extract the most important sentences from each document. In the second phase, the candidate sentences from the single documents are concatenated, and a multi-document extraction is performed to produce quickview summaries. Finally, in the third phase, multi-document abstractive models are built from the quickview results to generate more coherent and natural-sounding summaries. In this paper, the authors propose a method for multi-document summarization based on cluster similarity. The proposed method is based on a hybrid model that combines a modified version of the PageRank algorithm and a text correlation considerations mechanism. The method first clusters the documents based on their similarity and then generates summaries by selecting the most important sentences from each cluster. |
| Related Work | Two phase for generating a summary : the extractive phase for reducing redundancies and collecting important information, and abstractive phase for generating a natural language summaries <br />1.  (Yasunaga et al., 2017), the authros built a senteces-based graph where each node is a sentences and each edge is a relation between 2 sentences. Then they used GRU model for extracting vector representation for each sentences and fed the graph and the representation vector to a GCN for aggregating the final representation vector.<br />2. After the sentence embedding stage, they employed a second-level GRU to extract the entire cluster's vector representation, which was then combined with a sentence-level representation vector to estimate the salience of each sentence.<br /> 3. Wang et al. (2020) proposed the HeterDoc-SUM Graph method for multi-document summarization, which represents words, sentences, and documents as nodes in a graph. The graph contains nodes for sentences and documents, which are connected by nodes for containing words. The use of containing word nodes as bridges between sentence and document nodes enhances the relations that exist in the data, including sentence-sentence, sentence-document, and document-document relations. The built graph is fed into a graph attention network for summarization.<br />4. (Jin et al., 2020) proposes uses a Transformer model for capturing the semantic relationships across three levels of granularity, i.e., word, sentence, and document.The sentence-level information is used for extractive summarization, while the word-level information is used for abstractive summarization. A fusion gate is employed to integrate and update the semantic information, which focuses on important knowledge using a sparse-attention mechanism.| 

