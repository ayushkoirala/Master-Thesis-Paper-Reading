#1. Transformer & seq2se1 model for paraphrase Generation

#2. Paper Link : https://aclanthology.org/D19-5627.pdf

| Topic | Transformer & seq2se1 model for paraphrase Generation |
| ---------------| --------------------------- |
|Problem | Paraphrase generation lacks clarity and effectiveness in producing quality paraphrased sentences|
| Solution | The proposed solution is a framework that combines the transformer and sequence-to-sequence (seq2seq) models. The framework consists of two layers of encoders, where the first layer is a transformer model that captures long-term dependencies and semantic and syntactic properties of the input sentence, and the second layer is a seq2seq model with gated recurrent units. The rich vector representation generated by the transformer serves as input to the seq2seq model, producing a state vector for decoding, resulting in improved paraphrase generation performance as demonstrated by experimental results on the QUORA and MSCOCO datasets.|
| Introduction | The proposed solution is a novel framework that leverages the transformer model and seq2seq model (GRU). The multi-head self-attention mechanism in the transformer model enables it to learn long-range dependencies in the input sequence, while also mimicking the syntactic and semantic structure of the sentence. The GRU model is used to produce a fixed-size state vector for decoding, using the more qualitative learned vector representations from the transformer. This framework combines the strengths of both models to effectively generate paraphrased sentences. |
| Method | 3.1 Stacked Encoders <br /> 3.1.1 Encoder-Transformer: The proposed solution uses the transformer-encoder as a pretraining module for the input sentence. The transformer contains 6 stacked identical layers driven by self-attention and is used to learn a richer representation of the input vector that better handles long-term dependencies and captures the syntactic and semantic properties of the input sentence. This enriched representation is then used to obtain a fixed-state representation for decoding into the desired output sentence. <br /> 3.1.2 Encoder-GRU-RNN : The proposed architecture in this paper uses a single layer uni-directional GRU-RNN encoder to produce a fixed-state vector representation of the input sequence. The input to the GRU-RNN encoder is the output of the transformer. The GRU-RNN encoder uses equations to produce the fixed-state vector representation, which is then used for decoding into the desired output sentence.<br />![alt text](https://github.com/ayushkoirala/Master-Thesis-Paper-Reading/blob/main/image/image-1.png)  <br /> 3.2 Decoder-GRU-RNN : The fixed-state vector representation produced by the GRU-RNN encoder is used as the initial state for the decoder. The decoder at each time step receives the previous generated word and hidden state, and generates the output word based on a softmax probability calculation using the vocabulary words.|
| Experiment | 4.1 Baselines <br /> 1. VAE-SVG-EQ : It was Proposed in 2018, and was current state of art, variational autoencoder as its main component. <br /> 2. RbM-SL : It was evaluator trained by reinforncement learning. <br /> 3. Residual LSTM (2016) : It is stacked residual long short term memory networks(Lstm) <br /> 5. SEQ: Encoder-decoder frame work with single GRU-RNN encoder layer. |
| Implementation | 1. The paper uses pre-trained 300-dimensional GloVe word embeddings as the representation of input sentences.<br />2. The maximum sentence length is set to 15 and 10 for input and target sentences respectively.<br /> 3. The paper uses the transformer base hyperparameters but with a hidden size of 300 and dropout set to 0.0 for MSCOCO and 0.7 for QUORA datasets. <br /> 4. The GRU-RNN encoder and decoder both contained 300 hidden units.<br /> 5. The authors pre-processed the datasets and used a target vocabulary of approximately 15,000 words that occur at least twice. <br /> 6. The paper trains the model with a fixed learning rate of 0.0005, stops training when the validation loss does not decrease after 5 epochs, and uses greedy-decoding during training and validation with a maximum of 5 times the target sentence length. For testing/inference, beam-search decoding is used. |
| Dataset | 1. QUORA - (120 K) dataset. It contains questions pairs <br /> 2. MSCOCO contains free form texts which are human annotations of images. |
|Evaluation | For quantitative ananlysis: BLEU, ROUGE, METEOR are used. |
| Results: | [alt text](https://github.com/ayushkoirala/Master-Thesis-Paper-Reading/blob/main/image/image-2.png) <br/>  [alt text](https://github.com/ayushkoirala/Master-Thesis-Paper-Reading/blob/main/image/image-3.png)|