#1. Transformer & seq2se1 model for paraphrase Generation

#2. Paper Link : https://aclanthology.org/D19-5627.pdf

| Topic | Transformer & seq2se1 model for paraphrase Generation |
| ---------------| --------------------------- |
|Problem | Paraphrase generation lacks clarity and effectiveness in producing quality paraphrased sentences|
| Solution | The proposed solution is a framework that combines the transformer and sequence-to-sequence (seq2seq) models. The framework consists of two layers of encoders, where the first layer is a transformer model that captures long-term dependencies and semantic and syntactic properties of the input sentence, and the second layer is a seq2seq model with gated recurrent units. The rich vector representation generated by the transformer serves as input to the seq2seq model, producing a state vector for decoding, resulting in improved paraphrase generation performance as demonstrated by experimental results on the QUORA and MSCOCO datasets.|
| Introduction | The proposed solution is a novel framework that leverages the transformer model and seq2seq model (GRU). The multi-head self-attention mechanism in the transformer model enables it to learn long-range dependencies in the input sequence, while also mimicking the syntactic and semantic structure of the sentence. The GRU model is used to produce a fixed-size state vector for decoding, using the more qualitative learned vector representations from the transformer. This framework combines the strengths of both models to effectively generate paraphrased sentences. |
| Method | 3.1 Stacked Encoders <br /> 3.1.1 Encoder-Transformer: The proposed solution uses the transformer-encoder as a pretraining module for the input sentence. The transformer contains 6 stacked identical layers driven by self-attention and is used to learn a richer representation of the input vector that better handles long-term dependencies and captures the syntactic and semantic properties of the input sentence. This enriched representation is then used to obtain a fixed-state representation for decoding into the desired output sentence. <br /> 3.1.2 Encoder-GRU-RNN : The proposed architecture in this paper uses a single layer uni-directional GRU-RNN encoder to produce a fixed-state vector representation of the input sequence. The input to the GRU-RNN encoder is the output of the transformer. The GRU-RNN encoder uses equations to produce the fixed-state vector representation, which is then used for decoding into the desired output sentence.<br />![alt text](https://github.com/ayushkoirala/Master-Thesis-Paper-Reading/blob/main/image/image-1.png)  <br /> 3.2 Decoder-GRU-RNN : The fixed-state vector representation produced by the GRU-RNN encoder is used as the initial state for the decoder. The decoder at each time step receives the previous generated word and hidden state, and generates the output word based on a softmax probability calculation using the vocabulary words.| 