#1. Topic-Centric Unsupervised Multi-Document Summarization of Scientific and News Articles

#2. Paper Link : https://arxiv.org/pdf/2011.08072v1.pdf

| Topic | Topic-Centric Unsupervised Multi-Document Summarization of Scientific and News Articles |
| ---------------| --------------------------- |
| Problems |1. The increasing number of articles published in the research and media community has created a need for coherent, concise, informative, and grammatically correct summaries.<br />2. Abstractive summarization, which involves paraphrasing and fusing sentences to create new and informative summaries, poses challenges in capturing abstractive concepts shared among sentences across multiple documents.|
|Solution |The framework proposed in the paper consists of two phases: an extractive phase and an abstractive phase. In the extractive phase, a three-fold approach is followed: <br /> a. Core and peripheral articles are identified for each set of related articles.<br /> b. Clusters are instantiated using language units from the core article, and centroid-based clustering is performed to place language units from peripheral articles into the clusters.<br /> c. Language units within a cluster are fused using an enhanced Multi-Sentence Compression (MSC) technique, along with a novel algorithm, to maximize topical coverage and relevance.<br />In the abstractive summarization phase:<br />a. Abstractive Language Units (ALUs) are generated using GPT-2, a text generation model.<br />b. The generated ALUs are fused into an abstractive summary using MSC.|
|Method| A. Extractive phase:<br /><strong>Topic Modeling:</strong> For the MAG-20 dataset, groups of topically related abstracts for each Field of Study (FoS) are determined using LDA (Latent Dirichlet Allocation) topic models. The optimal number of topics is determined based on coherence scores, and the topics with the highest coherence score are used for Topical HAC (Hierarchical Agglomerative Clustering).<br /><strong>Topical Hierarchical Agglomerative Clustering:</strong> To address semantically redundant keywords within topics, topics with high similarity among their keywords are clustered using HAC. SciBERT embeddings are used to represent each keyword, and the number of clusters is determined using the Silhouette coefficient.<br/><strong>Core and Peripheral Articles Identification:</strong> The core article is identified within each cluster based on its similarity to other articles in the cluster. The Cross-Article Similarity Score is computed using doc2vec-based cosine similarity, and the article with the highest cumulative similarity is chosen as the core article. The remaining articles in the cluster are considered peripheral articles.<br /><strong>Centroid-based Clustering:</strong> Extractive Language Units (ELUs) are generated from both the core and peripheral articles. To preserve interdependence among sentences within a document, neural coreference resolution is used to identify and maintain interdependent sentences as a single ELU. ELUs from the core article initiate clusters, and ELUs from peripheral articles are placed into clusters based on cosine similarity between their embeddings and the embeddings of ELUs from the core article.<br /><strong>Multi-Sentence Compression:</strong> Word graphs are constructed for each cluster of ELUs. An algorithm is developed to extract paths from the word graph based on topical coverage and relevance. Paths that span at least two ELUs in the cluster are selected while maintaining a 100-word summary limit. Topical coverage measures how well a path covers the dominant topics discussed by the articles, and relevance measures the relevance of a path to the ELUs. The cumulative score of a path is determined by a weighted sum of topical coverage and relevance.<br /><strong>B. Abstractive phase:</strong><br /> Abstractive Language Unit (ALU) Generation: The abstractive phase begins by assuming that the title or headline of an article serves as an abstraction of the individual extractive language units (ELUs) within the same article. A method is proposed to generate an ALU for an ELU using the ELU and title/headline as prompts for generating text. Bidirectional encodings of the title/headline are combined with the ELU to enable the generation of abstractive text. For ELUs with multiple sentences, each sentence is encoded using sentence-BERT, and the representations are concatenated. T-SNE is used for dimensionality reduction of the ELU encoding. Sentence-BERT is also used for encoding the title/headline without dimensionality reduction. A GPT-2 model is fine-tuned for each field of study (FoS) and used to generate ALUs by providing a concatenation of the bidirectional encodings of the ELU and the title/headline as input. The fine-tuned GPT-2 model has 124M parameters and generates 10 candidate ALUs. The generated ALU that maximizes semantic similarity and minimizes syntactic similarity with the ELU is selected. An abstractiveness score is introduced to evaluate the ALUs based on their semantic and syntactic similarity with the ELU.<br />Multi-Sentence Compression: After generating ALUs for a cluster, a word graph is constructed, and the same Multi-Sentence Compression (MSC) algorithm used in the extractive phase is applied. The MSC algorithm ranks the paths in the word graph and selects informative paths based on topical coverage and relevance. The same ranking formulation and path selection algorithm are used to select informative paths from the word graph built from a cluster of ALUs. The selected paths form the final abstractive summary.|
