# `Prompt Combines Paraphrase: Teaching Pre-trained Models to Understand Rare Biomedical Words`

Paper Link : https://aclanthology.org/2022.coling-1.122.pdf


| 1. Topic | Prompt Combines Paraphrase: Teaching Pre-trained Models to Understand Rare Biomedical Words |
|-----|--------------------------|
| Problems | The problem mentioned in the prompt is that prompt-based fine-tuning of pre-trained models has been effective for many NLP tasks in general domains, but has not been thoroughly investigated in the biomedical domain. Biomedical words are often rare in the general domain, but common in biomedical contexts, leading to decreased performance of pre-trained models even after fine-tuning, particularly in low-resource scenarios. |
| Solution | The proposed solution is a simple yet effective approach to help models learn rare biomedical words during prompt-based fine-tuning. The approach does not require any extra parameters or training steps and uses few-shot vanilla prompt settings. The method involves augmenting the prompt with the definition of rare biomedical words to help the model learn their meaning and usage. |
| Introduction | 1. Problem with the pre-trained models for biomedical NLP tasks involves like limited data and rare biomedical words <br /> 2. The author purpose a novel method to combine the prompt paradigm and paraphrases of rare biomedical words in the tuning stage of pre-trained models to address the limitation caused by "rare but key words" in biomedical texts. |
| Related Work | <strong>1. Representation learning of rare words in pretrained models :</strong> <br />- The vocabulary list of words in natural language processing (NLP) typically follows a Zipf distribution, which means that a few high-frequency words dominate the majority of the language while the vast majority of words are rarely used. This distribution can create challenges for pre-trained models, as high-frequency words can dominate the representation of a sentence, leading to semantic bias.<br />- Both rare and high-frequency words can limit the performance of pre-trained models. Rare words can be decisive for sentence understanding, but their infrequency can make it challenging for models to learn their meaning and usage. To address this, researchers have proposed various solutions, such as approximating embeddings for rare words using a single token or maintaining a note dictionary of rare words to save contextual information during pre-training. These approaches can help enhance the representation of rare words in pre-trained models and improve their performance in downstream NLP tasks.<br /><strong>2. Biomedical pre-trained models</strong> <br />- One point is that with the increasing use of pre-trained models in NLP, there has been growing interest in investigating their performance in the biomedical domain. To address the unique challenges of this domain, researchers have developed domain-specific vocabularies, guided pre-trained models with domain knowledge, and learned clinical word embeddings with the help of synonyms in the Unified Medical Language System (UMLS) Metathesaurus. These efforts have the potential to improve the accuracy and effectiveness of pre-trained models for biomedical NLP tasks.<br /><strong>3.Tuning pre-trained models with prompt</strong><br />- There are two main types of prompts that have been used in fine-tuning: discrete prompts in natural language and continuous prompts in representation based on trainable vectors. Discrete prompts convert downstream tasks into cloze question formats without requiring additional parameters, while continuous prompts insert prompt embeddings into models, which can perform better but require additional training costs and reduce explainability.<br />- The recent work has explored training continuous prompts as a parameter-efficient method rather than tuning the parameters of the entire pre-trained model. However, in this study, the researchers chose to use the paradigm of discrete prompts to avoid introducing ambiguity from prompt embeddings or incurring additional training costs on extra data. |
| Method | <strong>1. Rare Words </strong > <br />- The definition of rare words in this work is based on their frequency in the pre-training corpora, which can vary from context to context. This means that a word that is rare in one corpus may not be rare in another corpus or in a downstream task, highlighting the importance of context-specific rare word identification.<br />- Tokenizers used by pre-trained models, such as byte-pair encoding or WordPiece, split words into sub-words based on their frequency or likelihood, which is dominated by common words. As a result, rare words may be split into non-rare tokens, and the original semantics of rare words may not be preserved after tokenization. Additionally, different pre-trained models may tokenize the same rare word into different tokens, making rare tokens model-specific.<br /> <strong>2. Selection of Rare Biomedical Words</strong><br />- The authors use three widely used biomedical corpora - PubMed abstract, PubMed Central (PMC) full-text, and MIMIC-III dataset to obtain the frequency of each word in the pre-training phase, and consider rare words from biomedical domain due to their domain-specific distribution and task-specific importance.<br />-The authors retrieve the paraphrases of the rare biomedical words from an online dictionary called "Wiktionary" and only keep the rare words that are tagged with medical-related categories such as medical, medicine, disease, symptom, and pharmacology to optimize the selection.<br /><strong> 3.  Selection of Paraphrases</strong><br />- The authors exclude rare biomedical words with more than one corresponding paraphrase to avoid introducing noise from inappropriate paraphrases. This helps to ensure that the chosen paraphrase is appropriate for the specific context.<br />-The authors also ignore paraphrases that contain additional rare words whose frequencies are below the set threshold, as this only replaces one rare word with another and does not add much semantic value. This helps to ensure that the selected paraphrases contain words that are frequent enough to have meaningful semantic information.<br /><strong>4. Prompt-based Fine-Tuning with Paraphrases</strong><br />- The authors use the idea of seeking dictionaries for the corresponding paraphrases when encountering new words to guide the pre-trained models with paraphrases.<br />- By providing paraphrases of biomedical rare words as a portable plug-in module, the pre-trained models can generate them for any dataset instantly before prompt-based fine-tuning.|